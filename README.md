### Hello there!,Welcome to my Github Profile. Great to see you here! üëã üòÉ

I'm Dushyanth Reddy, a graduate with a Master's in Data Science at Indiana University Bloomington,Indiana [https://luddy.indiana.edu/index.html], were i developed a strong foundation in machine learning, statistics, and programming languages such as Python and its' various frameworks and libraries such as Numpy, Pandas, SciKit Learn,PyTroch, R, Spark and Postgresql. I have worked as Research Assistant in GARYFALLIDIS RESEARCH GROUP[https://grg.luddy.indiana.edu//]. I'm a Machine Learning Engineer [LLM's] üë®‚Äçüíª  with a passion for Natural Language Processing (NLP), Large Language Models (LLMs), Retrieval-Augmented Generation (RAGs) and AI Agents (AGI), working on multiple projects mainly DQA Interview Chatbot using Langchain and LlamaIndex.<br/>

üëØ I‚Äôm looking to collaborate on your open source projects. <br/>
üì´ How to reach me: dushyanthteddy5089@gmail.com <br/>


![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=dssrb&show_icons=true)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=dssrb&layout=compact)

## Research interests
My research interests lie at the Conversational AI and Multimodal LLMs. 

## üíª LLM Expertise
Model Architectures: Transformer, BERT, GPT, T5, BART<br/>
Training & Fine-tuning: Pre-training, Fine-tuning, Few-shot learning, Zero-shot learning<br/>
Optimization Techniques: Mixed-precision training, Gradient accumulation, Distributed training<br/>
Efficiency Methods: Quantization, Pruning, Knowledge distillation, Parameter-efficient fine-tuning (LoRA, Adapter)<br/>
Prompt Engineering: In-context learning, Chain-of-thought prompting, Instruction tuning<br/>
Evaluation: Perplexity, BLEU, ROUGE, BERTScore, Human evaluation design<br/>

## üõ†Ô∏è Technical Stack
This comprehensive technical stack covers the tools, frameworks, and platforms commonly used in LLM development, training, and deployment.<br/>
Programming: Python, C++, CUDA <br/>
ML & NLP Libraries: Numpy, Scipy, Matplotlib, Seaborn, Scikit-learn, NLTK, Spacy <br/>
Deep Learning Frameworks: PyTorch <br/>
LLM-specific Libraries: Hugging Face Transformers, Langchain, LlamaIndex <br/>
Distributed Computing: Horovod, Ray, Dask <br/>
Model Optimization: ONNX, TensorRT, Brevitas, Neural Magic <br/>
Data Processing: Apache Spark, Dask, NLTK and spaCy <br/>
Experiment Tracking and Visualization: MLflow, W&B, TensorBoard <br/>
Hyperparameter Optimization: Optuna, Ray Tune <br/>
Model Serving: TorchServe, Triton Inference Server <br/>
Containerization and Orchestration: Docker, Kubernetes, Kubeflow <br/>
Cloud Platforms: AWS <br/>
Version Control and Collaboration: Git, DVC, GitHub/GitLab <br/>
Monitoring and Logging: Prometheus, Grafana, Elasticsearch, Logstash, Kibana <br/>
Testing and Quality Assurance: PyTest, Great Expectations, Locust <br/>
Ethics and Responsible AI: AI Fairness 360, Captum, SHAP (SHapley Additive exPlanations) <br/>

## Kaggle
https://www.kaggle.com/dushyanthreddybonthu

## Articles
[MEDIUM] https://medium.com/@dushyanthreddy5089/slms-small-language-models-f18b4f198156
